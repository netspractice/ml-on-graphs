{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment â€” Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch -q\n",
    "!pip install dgl==0.6 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl import function as fn\n",
    "from dgl.nn import GATConv, SAGEConv\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from zlib import adler32\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the basics of PyTourch. Skip this section if you are familiar with this material. \n",
    "\n",
    "We want to find a function that as close as possible to the sin with some noise \n",
    "\n",
    "$$y = \\sin(x) + \\varepsilon$$\n",
    "\n",
    "First, let us generate the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "X = np.linspace(-10, 10, N)\n",
    "np.random.seed(0)\n",
    "y = np.sin(X) + np.random.normal(0, 0.2, N)\n",
    "X = minmax_scale(X, (-1, 1))\n",
    "y = minmax_scale(y, (-1, 1))\n",
    "plt.scatter(X, y, s=3)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert np.arrays into tensors via `torch.FloatTensor` (float type) or `torch.LongTensor` (int type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.FloatTensor(X[:, None])\n",
    "y_tensor = torch.FloatTensor(y[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model, here is a simple shallow perceptron with layer sizes $1 \\to 64 \\to 64 \\to 1$ so that\n",
    "\n",
    "$$y = f(x)$$\n",
    "\n",
    "where $f$ is a neural net, $x$ is a given input real value, $y$ is a output real value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the loss function be mean squeared error between the true dependency and approximation.\n",
    "\n",
    "$$\\text{Error} = \\frac{1}{N}\\sum_i^N \\left(\\sin(x_i) + \\varepsilon - f(x_i)\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train loop constists of: computing loss, grad, making a step to the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    y_pred = model.forward(X_tensor)\n",
    "    loss = MSE(y_pred, y_tensor)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    plt.scatter(X, y, s=3)\n",
    "    plt.plot(X, y_pred.detach().numpy(), linewidth=2, c='tab:orange')\n",
    "    plt.title('Epoch: {}/{}, Loss: {:.4f}'.format(i+1, n_epochs, loss.item()))\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Graph convolutional network (1 point)\n",
    "\n",
    "In the task, we will train the Graph Convolutional Network (GCN) model to predict the node label. In the cora dataset, to predict the category of the paper. Let us build the cora dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = data[0]\n",
    "type(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an one-hot encoded feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = G.ndata['feat']\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And labels of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = G.ndata['label']\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test masks are tensors with boolean values that define which nodes are train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = G.ndata['train_mask']\n",
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = G.ndata['test_mask']\n",
    "test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in-degree nodes will lead to invalid output value. This is because no message will be passed to those nodes, the aggregation function will be appied on empty input. A common practice to avoid this is to add a self-loop for each node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.add_self_loop(data[0])\n",
    "G.adjacency_matrix().to_dense()[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph convolution mathematically is defined as follows:\n",
    "\n",
    "$$h_i^{l+1} = \\sigma\\left(b^{l} + \\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_j^{l}W^{l}\\right)$$\n",
    "\n",
    "where $\\mathcal{N}(i)$ is the set of neighbors of node $i$,$c_{ij}$ is the product of the square root of node degrees (i.e.,  $c_{ij} = \\sqrt{|\\mathcal{N}(i)|}\\sqrt{|\\mathcal{N}(j)|}$), and $\\sigma$ is an activation function.\n",
    "\n",
    "\n",
    "The `GCNLayer` works as follows:\n",
    "\n",
    "1. Apply linear layer to the hidden state\n",
    "2. Multiply the result by the normalization `self.norm`\n",
    "3. Aggregate data over neighbourhood multiplying adjacency matrix by the result\n",
    "4. Multiply the result by the normalization `self.norm`\n",
    "5. Return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "292e0296612d36398c832f4e0f0127c5",
     "grade": false,
     "grade_id": "cell-49c0a67459fef8d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g: nx.Graph,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 dropout: float = None):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        adj = nx.to_numpy_array(g)\n",
    "        self.adj = torch.FloatTensor(adj)\n",
    "        norm = 1 / adj.sum(axis=0)**(1/2)\n",
    "        self.norm = torch.FloatTensor(norm)[:, None]\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "    def forward(self, h):\n",
    "        if self.dropout and self.training:\n",
    "            h = self.dropout(h)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5588b99536f5b879aae2e80e339e6cba",
     "grade": true,
     "grade_id": "cell-3773092e16e3398b",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "layer = GCNLayer(G.to_networkx(), in_feats=100, out_feats=10, dropout=0.1)\n",
    "with torch.no_grad():\n",
    "    assert layer(torch.randn(100)).shape == (2708, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the class `GCN`. Use `GCNLayer` layers with sizes $1433 \\to 16 \\to 7$. Here is the zero dropout in the first layer, and the given dropout in the second one. Use the `F.relu` activation after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1a5b010d9f3dbdb30fb4609a01d5e0d",
     "grade": false,
     "grade_id": "cell-5e0c138f5ccaea0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, g, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "\n",
    "    def forward(self, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97fa746d13f86782e184ce5c10a2ab43",
     "grade": true,
     "grade_id": "cell-fe037acfca3092bc",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GCN(G.to_networkx(), dropout=0.1)\n",
    "assert adler32(str(model).encode()) == 4004464339\n",
    "with torch.no_grad():\n",
    "    assert model(torch.randn(1433)).shape == (2708, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks we will use `CrossEntropy` loss. Change an optimizer if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop. To speed up calculation test loss, use `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1af8c71a01b3ffc9d764764eefa90fed",
     "grade": false,
     "grade_id": "cell-859836ad344792e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    #test_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss, test_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccb2481e6102d29afe739d16de23c39f",
     "grade": true,
     "grade_id": "cell-0dbd2da971a339fe",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "logits = model.forward(features)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(labels[test_mask], y_pred)\n",
    "assert score > 0.75\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Graph autoencoder (2 points)\n",
    "\n",
    "In the previous task, we have trained our model with supervised loss in the node classification task.\n",
    "\n",
    "One can train GNN in unsupervised fashion. To do so we can state our problem as a graph autoencoder. We will train embeddings in the way to reconstruct adjacency matrix.\n",
    "\n",
    "We will decode our adjacency matrix with `InnerProductDecoder` class.\n",
    "\n",
    "You need to implement `forward` function. It works as follows:\n",
    "\n",
    "1. Apply dropout layer if dropout rate > 0 and not None and we are in train mode.\n",
    "2. Multiply embeddings matrix and transposed embedding matrix\n",
    "3. Apply activation if it is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e41e8f0e3a45d36ed59274a66c229085",
     "grade": false,
     "grade_id": "cell-31af15ac93053cc6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class InnerProductDecoder(nn.Module):\n",
    "    def __init__(self, activation=torch.sigmoid, dropout=0.1):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, h):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f049476539ac5d2f0441973a8901eb14",
     "grade": true,
     "grade_id": "cell-5bd597fc40f459f5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "decoder = InnerProductDecoder(torch.sigmoid, dropout=None)\n",
    "adj = decoder(torch.Tensor(np.arange(10).reshape(5, 2)))\n",
    "assert round(float(adj.numpy()[0, 0]), 4) == 0.7311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReconstructionLoss` function takes dgl.Graph as a parameter. After initialization, it should extract adjacency from the graph as a dense tensor, and calculate `pos_weight` as $(N^2 - E) / E$, where $N$ is a number of nodes and $E$ is a number of edges.\n",
    "\n",
    "Implement `__call__` function that calculates `F.binary_cross_entropy_with_logits` between predicted_adjacency and real adjacency of a graph with `pos_weight`.\n",
    "\n",
    "*Hint: to obtain dense adjacency matrix, use `g.adjacency_matrix().to_dense()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4983e4aeb2e2ca974d9deb827250bda",
     "grade": false,
     "grade_id": "cell-f21ed6c1f82b2b7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ReconstructionLoss:\n",
    "    def __init__(self, g):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __call__(self, predicted_adjacency):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1d94ec53bff65852297ad8a636f956e",
     "grade": true,
     "grade_id": "cell-dc7a5b14910f1b1b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rec_loss = ReconstructionLoss(G)\n",
    "assert round(rec_loss(G.adjacency_matrix().to_dense()).item(), 3) == 1.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop so that node embeddings obtained from the GCN model are fed to the decoder, reconstruction loss of a predicted adjacency matrix is computed and then an optimization step is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80c7511c786db28289a9735042a989b0",
     "grade": false,
     "grade_id": "cell-ab892da4f2b04e92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "log = []\n",
    "\n",
    "model = nn.Sequential(\n",
    "    GCNLayer(G.to_networkx(), 1433, 16, 0.1),\n",
    "    nn.ReLU(),\n",
    "    GCNLayer(G.to_networkx(), 16, 16, 0.5),\n",
    ")\n",
    "decoder = InnerProductDecoder()\n",
    "rec_loss = ReconstructionLoss(G)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will validate how unsupervised embeddings will work for node classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d101c92e14f577429903ce014a8b727",
     "grade": true,
     "grade_id": "cell-08af8fff077a7121",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model.eval()\n",
    "emb = model.forward(features).data.numpy()\n",
    "labs = labels.numpy()\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(emb[train_mask], labs[train_mask])\n",
    "\n",
    "y_true = labs[test_mask]\n",
    "y_pred = lr.predict(emb[test_mask])\n",
    "score = balanced_accuracy_score(y_true, y_pred)\n",
    "assert score > 0.4\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Graph attention network (3 point)\n",
    "\n",
    "Let us apply Graph Attention Network over an input signal.\n",
    "\n",
    "$$h_i^{(l+1)} = \\sum_{j\\in \\mathcal{N}(i)} \\alpha_{i,j} W^{(l)} h_j^{(l)}$$\n",
    "\n",
    "where $\\alpha_{ij}$ is the attention score bewteen node $i$ and node $j$:\n",
    "\n",
    "$$\\alpha_{ij}^{l} = \\text{Softmax}_{i} (e_{ij}^{l})$$\n",
    "\n",
    "$$e_{ij}^{l} = \\text{LeakyReLU}\\left(\\vec{a}^T [W h_{i} \\| W h_{j}]\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()\n",
    "G = data[0]\n",
    "G = dgl.add_self_loop(G)\n",
    "features = G.ndata['feat']\n",
    "labels = G.ndata['label']\n",
    "train_mask = G.ndata['train_mask']\n",
    "test_mask = G.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `GAT`. Use `GATConv` layers:\n",
    "`GATConv(1433, 8, num_heads=8)` $\\to$ `GATConv(8*8, 7, num_heads=1)`\n",
    "\n",
    "In `forward` use \n",
    "* `tensor.view(-1, m * n)` to reshape, where `m` is the number of heads, `n` is the number of output dimensions\n",
    "* `F.elu` activation after reshape\n",
    "* `tensor.squeeze` to decrease shape `(n_features, 1, out_dim) -> (n_features, out_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdc8345eb40be185359aeb01fff87fde",
     "grade": false,
     "grade_id": "cell-2ac8340166410ded",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, G, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7cf6488822a1295f5f402e80ca25a0c",
     "grade": true,
     "grade_id": "cell-8b1838d547caf32d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GAT()\n",
    "assert str(model) == 'GAT(\\n  (conv1): GATConv(\\n    (fc): Linear(in_features=1433, out_features=64, bias=False)\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (attn_drop): Dropout(p=0.0, inplace=False)\\n    (leaky_relu): LeakyReLU(negative_slope=0.2)\\n  )\\n  (conv2): GATConv(\\n    (fc): Linear(in_features=64, out_features=7, bias=False)\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (attn_drop): Dropout(p=0.0, inplace=False)\\n    (leaky_relu): LeakyReLU(negative_slope=0.2)\\n  )\\n)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), \n",
    "                       lr=0.005, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop. To speed up calculation test loss, use `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d409df1ea0e998208b32f57cfb9b814",
     "grade": false,
     "grade_id": "cell-8d019b9371d11f59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    #test_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss, test_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3b5d5976d418d83fb7ca869601a8f74",
     "grade": true,
     "grade_id": "cell-f0b5cbfec9b4510d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "logits = model.forward(G, features)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(labels[test_mask], y_pred)\n",
    "assert score > 0.7\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. GraphSAGE (4 points)\n",
    "\n",
    "Consider GraphSAGE, a representation learning technique suitable for dynamic graphs. GraphSAGE is capable of predicting embedding of a new node, without requiring a re-training procedure. To do so, GraphSAGE learns aggregator functions that can induce the embedding of a new node given its features and neighborhood. This is called inductive learning.\n",
    "\n",
    "$$h_{\\mathcal{N}(i)}^{(l+1)} = \\mathrm{aggregate}\\left(\\{h_{j}^{l}, \\forall j \\in \\mathcal{N}(i) \\}\\right)$$\n",
    "$$h_{i}^{(l+1)} = \\sigma \\left(W \\cdot \\text{concat}(h_{i}^{l}, h_{\\mathcal{N}(i)}^{l+1}) \\right)$$\n",
    "$$h_{i}^{(l+1)} = \\mathrm{norm}(h_{i}^{l})$$\n",
    "\n",
    "Aggregator types here can be `mean`, `gcn`, `pool`, `lstm`. Consider GraphSAGE on the Karate Club graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "labels = [1 if i=='Mr. Hi' else 0 for i in nx.get_node_attributes(G, 'club').values()]\n",
    "labels = torch.LongTensor(labels)\n",
    "features = torch.FloatTensor(np.arange(0, 34)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(\n",
    "    G, with_labels=True, \n",
    "    node_color=['tab:orange' if i==1 else 'tab:blue' for i in labels], \n",
    "    cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us delete the node 31, train a model and then return it and predict its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(34)\n",
    "idx = idx[idx != 31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[idx]\n",
    "features = features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(\n",
    "    G.subgraph(idx), with_labels=True, \n",
    "    node_color=['tab:orange' if i==1 else 'tab:blue' for i in labels], \n",
    "    cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let choose test and train nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = [31, 32, 0, 11, 13, 2, 23, 29, 8]\n",
    "train_idx = list(set(np.arange(33)).difference(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the graph, test nodes are gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_color = np.ones((33, 3))\n",
    "node_color[labels == 0] = plt.cm.tab10(0)[:3]\n",
    "node_color[labels == 1] = plt.cm.tab10(1)[:3]\n",
    "node_color[test_idx] = (0.9, 0.9, 0.9)\n",
    "\n",
    "nx.draw_kamada_kawai(G.subgraph(idx), with_labels=True, \n",
    "                     node_color=node_color, cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dgl graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_graph = dgl.from_networkx(G.subgraph(idx))\n",
    "initial_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `SAGE`. Use `SAGEConv` layers with sizes $1 \\to 16 \\to 2$ and `mean` aggregation function. Put `F.relu` activation after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9399531298f5c0c2a65de6d6a0a15776",
     "grade": false,
     "grade_id": "cell-c83ca7ab5833bc2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, graph, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b64d5415e2e58b1bcd0641c27cc528d5",
     "grade": true,
     "grade_id": "cell-a7513e546c544396",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SAGE()\n",
    "assert str(model) == 'SAGE(\\n  (conv1): SAGEConv(\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (fc_self): Linear(in_features=1, out_features=16, bias=True)\\n    (fc_neigh): Linear(in_features=1, out_features=16, bias=True)\\n  )\\n  (conv2): SAGEConv(\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (fc_self): Linear(in_features=16, out_features=2, bias=True)\\n    (fc_neigh): Linear(in_features=16, out_features=2, bias=True)\\n  )\\n)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    logits = model.forward(initial_graph, features)\n",
    "    loss = CrossEntropy(logits[train_idx], labels[train_idx])\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(initial_graph, features)\n",
    "        test_loss = CrossEntropy(logits[test_idx], labels[test_idx])\n",
    "    \n",
    "    log.append([loss.item(), test_loss.item()])\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    \n",
    "    y_pred = torch.argmax(logits, 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    nx.draw_kamada_kawai(\n",
    "        G.subgraph(idx), with_labels=True, \n",
    "        node_color=['tab:orange' if i==1 else 'tab:blue' for i in y_pred], \n",
    "        cmap=plt.cm.tab10)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that prediction for the node 31 is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dab650bfd86a91be05e721cd00e089e",
     "grade": true,
     "grade_id": "cell-41e44fe2c4bf18a1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "graph = dgl.from_networkx(G)\n",
    "labels = [1 if i=='Mr. Hi' else 0 for i in nx.get_node_attributes(G, 'club').values()]\n",
    "labels = torch.LongTensor(labels)\n",
    "features = torch.FloatTensor(np.arange(0, 34)[:, None])\n",
    "predictions = torch.argmax(model(graph, features), 1)\n",
    "assert predictions[31] == labels[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
